{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf100
{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;\f2\froman\fcharset0 Times-Roman;
\f3\froman\fcharset0 Times-Bold;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww28600\viewh16580\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 InputLayer Class:
\f1\b0  This class is t\cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 he first layer of a neural network (the input layer), it is followed by one or more hidden layers.  It has an Attribut shape wich will be used for the first layer.\
\

\f0\b Abstract ConvolutionalLayer Class: 
\f1\b0 In this class convolutional layer the input keeps its original shape in order to exploit this correlation between the pixels. \
\

\f0\b CPUConvolutionalLayer Class:
\f1\b0  this class inherits from the Abstract ConvolutionalLayer Class using the CPU\'92s implementation by using the CpuWorker Class\
\

\f0\b FPGAConvolutionalLayer Class:
\f1\b0  \cf2 \outl0\strokewidth0 this class inherits from the Abstract ConvolutionalLayer Class using the FPGA\'92s implementation by using the FpgaWorker class\
\

\f0\b MaxPoolLayer Class: 
\f2\b0 \cf2 \outl0\strokewidth0 \strokec2 These layer take the output feature maps from the convolutional layers, and its Max function is to reduce the size of the input feature maps to reduce the amount of parameters in the network.\
The pooling layer works in a similar way to the convolutional layer since it operates on a subregion on the input feature maps and performs a filter operation.\
\

\f3\b ResponseNormalizationLayer Class: 
\f2\b0 The Class Response Normalization is kept separate from the others since it may require multiple memory access patterns.\
\

\f3\b DenseLayer Class:  
\f2\b0 this layer can use convolutional blocks for calculations, both of them have access to these blocks.\
\

\f3\b TrainableLayer Class: 
\f2\b0 this class to train the numeral network using a weight matrix
\f3\b \
\
ActivationFunction Class:  
\f2\b0 Activation function is introduced, its purpose is to allow small changes in the weights or bias to only cause a small change on the output, this property is helpful when training a network.\
\

\f3\b Sigmoid Class: 
\f2\b0 sigmoid is an activation function.  Same as the linear classifier this network also has ten outputs since it is the same classification problem, however the hidden layer has 25 neurons.
\f3\b \
\
Tanh Class: 
\f2\b0 is an activation function
\f3\b \
\
Sofmax Class: 
\f2\b0 \cf2 \outl0\strokewidth0 is an activation function
\f3\b \cf2 \outl0\strokewidth0 \strokec2 \
\
ReLU Class:  
\f2\b0 \cf2 \outl0\strokewidth0 is an activation function
\f3\b \cf2 \outl0\strokewidth0 \strokec2 \
\

\f2\b0 \

\f1 \cf2 \outl0\strokewidth0 \
\
\
}